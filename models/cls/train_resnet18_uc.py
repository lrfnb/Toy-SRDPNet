import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from collections import defaultdict
import re
from datetime import datetime


class UCMercedDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        """
        UCMercedæ•°æ®é›†åŠ è½½å™¨
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½•
            transform: æ•°æ®é¢„å¤„ç†å˜æ¢
        """
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []
        self.classes = []
        self.class_to_idx = {}

        # æ‰«ææ•°æ®ç›®å½•ï¼Œæå–å›¾ç‰‡å’Œæ ‡ç­¾
        self._load_dataset()

    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†ï¼Œä»æ–‡ä»¶åæå–æ ‡ç­¾"""
        if not os.path.exists(self.data_dir):
            raise ValueError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")

        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = []
        for file in os.listdir(self.data_dir):
            if file.lower().endswith(('.tif', '.jpg', '.jpeg', '.png')):
                image_files.append(file)

        if len(image_files) == 0:
            raise ValueError(f"åœ¨ç›®å½• {self.data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")

        # ä»æ–‡ä»¶åæå–ç±»åˆ«æ ‡ç­¾
        class_names = set()
        for filename in image_files:
            # æå–ç±»åˆ«åï¼ˆå»æ‰æ•°å­—å’Œæ‰©å±•åï¼‰
            # ä¾‹å¦‚: agricultural00.tif -> agricultural
            class_name = re.sub(r'\d+\..*$', '', filename)
            class_names.add(class_name)

        # åˆ›å»ºç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„
        self.classes = sorted(list(class_names))
        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.classes)}

        print(f"å‘ç° {len(self.classes)} ä¸ªç±»åˆ«: {self.classes}")

        # åˆ›å»ºæ ·æœ¬åˆ—è¡¨
        for filename in image_files:
            class_name = re.sub(r'\d+\..*$', '', filename)
            class_idx = self.class_to_idx[class_name]
            image_path = os.path.join(self.data_dir, filename)
            self.samples.append((image_path, class_idx))

        print(f"æ€»å…±åŠ è½½äº† {len(self.samples)} ä¸ªæ ·æœ¬")

        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        class_counts = defaultdict(int)
        for _, label in self.samples:
            class_counts[self.classes[label]] += 1

        print("å„ç±»åˆ«æ ·æœ¬æ•°ç»Ÿè®¡:")
        for class_name, count in sorted(class_counts.items()):
            print(f"  {class_name}: {count}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image_path, label = self.samples[idx]

        # åŠ è½½å›¾ç‰‡
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"æ— æ³•åŠ è½½å›¾ç‰‡ {image_path}: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„é»‘è‰²å›¾ç‰‡
            image = Image.new('RGB', (256, 256), (0, 0, 0))

        if self.transform:
            image = self.transform(image)

        return image, label


def create_data_loaders(data_dir, batch_size=32, train_split=0.7):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨ (70% è®­ç»ƒ, 30% éªŒè¯)"""

    # æ•°æ®é¢„å¤„ç†
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.3),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½å®Œæ•´æ•°æ®é›†
    full_dataset = UCMercedDataset(data_dir, transform=None)

    # åˆ†å‰²æ•°æ®é›† (70% è®­ç»ƒ, 30% éªŒè¯)
    dataset_size = len(full_dataset)
    train_size = int(train_split * dataset_size)
    val_size = dataset_size - train_size

    print(f"\næ•°æ®é›†åˆ†å‰²:")
    print(f"è®­ç»ƒé›†: {train_size} æ ·æœ¬ ({train_split * 100:.0f}%)")
    print(f"éªŒè¯é›†: {val_size} æ ·æœ¬ ({(1 - train_split) * 100:.0f}%)")

    # éšæœºåˆ†å‰²
    torch.manual_seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size]
    )

    # è®¾ç½®ä¸åŒçš„å˜æ¢
    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=4, pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False,
        num_workers=4, pin_memory=True
    )

    return train_loader, val_loader, full_dataset.classes


def create_model(num_classes, pretrained=True):
    """åˆ›å»ºResNet18æ¨¡å‹"""
    model = models.resnet18(pretrained=pretrained)

    # ä¿®æ”¹æœ€åä¸€å±‚ä»¥é€‚åº”ç±»åˆ«æ•°
    model.fc = nn.Linear(model.fc.in_features, num_classes)

    return model


def save_model(model, classes, best_val_acc, save_dir, epoch, optimizer=None):
    """ä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„"""
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # æ¨¡å‹æ–‡ä»¶å
    model_filename = f"resnet18_ucmerced_best_acc{best_val_acc:.2f}_{timestamp}.pth"
    model_path = os.path.join(save_dir, model_filename)

    # ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯
    save_dict = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'classes': classes,
        'class_to_idx': {class_name: idx for idx, class_name in enumerate(classes)},
        'num_classes': len(classes),
        'best_val_acc': best_val_acc,
        'model_architecture': 'resnet18',
        'timestamp': timestamp
    }

    if optimizer is not None:
        save_dict['optimizer_state_dict'] = optimizer.state_dict()

    torch.save(save_dict, model_path)

    # åŒæ—¶ä¿å­˜ä¸€ä¸ªæœ€æ–°çš„æ¨¡å‹ï¼ˆä¾¿äºåŠ è½½ï¼‰
    latest_path = os.path.join(save_dir, "resnet18_ucmerced_latest.pth")
    torch.save(save_dict, latest_path)

    return model_path


def train_model(model, train_loader, val_loader, classes, save_dir, num_epochs=50, learning_rate=0.001):
    """è®­ç»ƒæ¨¡å‹"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    model = model.to(device)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

    # è®­ç»ƒå†å²è®°å½•
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    best_val_acc = 0.0
    best_epoch = 0
    best_model_path = None

    print(f"\nå¼€å§‹è®­ç»ƒï¼Œæ¨¡å‹å°†ä¿å­˜åˆ°: {save_dir}")
    print("=" * 80)

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 40)

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()

            if batch_idx % 10 == 0:
                print(f'  Batch {batch_idx:3d}/{len(train_loader)} | '
                      f'Loss: {loss.item():.4f} | '
                      f'Acc: {100.0 * train_correct / train_total:.2f}%', end='\r')

        print()  # æ¢è¡Œ

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)

                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()

        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_acc = 100.0 * train_correct / train_total
        val_acc = 100.0 * val_correct / val_total

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)

        print(f'è®­ç»ƒ - Loss: {avg_train_loss:.4f} | Acc: {train_acc:.2f}%')
        print(f'éªŒè¯ - Loss: {avg_val_loss:.4f} | Acc: {val_acc:.2f}%')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch + 1
            best_model_path = save_model(model, classes, best_val_acc, save_dir, epoch, optimizer)
            print(
                f'ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (æå‡ {val_acc - best_val_acc + (val_acc - best_val_acc):.2f}%)')
            print(f'   æ¨¡å‹å·²ä¿å­˜: {best_model_path}')

        print(f'å½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (Epoch {best_epoch})')

        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        print(f'å­¦ä¹ ç‡: {current_lr:.6f}')
        print("=" * 80)

    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (Epoch {best_epoch})")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„: {best_model_path}")

    return model, {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies,
        'best_val_acc': best_val_acc,
        'best_epoch': best_epoch,
        'best_model_path': best_model_path
    }


def plot_training_history(history, save_dir):
    """ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒå†å²"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    epochs = range(1, len(history['train_losses']) + 1)

    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, history['train_losses'], 'b-', label='Train Loss', linewidth=2)
    ax1.plot(epochs, history['val_losses'], 'r-', label='Validation Loss', linewidth=2)
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)

    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, history['train_accuracies'], 'b-', label='Train Accuracy', linewidth=2)
    ax2.plot(epochs, history['val_accuracies'], 'r-', label='Validation Accuracy', linewidth=2)
    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)

    # æ ‡æ³¨æœ€ä½³å‡†ç¡®ç‡ç‚¹
    best_epoch = history['best_epoch']
    best_acc = history['best_val_acc']
    ax2.scatter(best_epoch, best_acc, color='red', s=100, zorder=5)
    ax2.annotate(f'Best: {best_acc:.2f}%\n(Epoch {best_epoch})',
                 xy=(best_epoch, best_acc), xytext=(10, 10),
                 textcoords='offset points', fontsize=10,
                 bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.7),
                 arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    plot_path = os.path.join(save_dir, 'training_history.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"è®­ç»ƒå†å²å›¾åƒå·²ä¿å­˜: {plot_path}")

    plt.show()


def load_model(model_path, device=None):
    """åŠ è½½ä¿å­˜çš„æ¨¡å‹"""
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    checkpoint = torch.load(model_path, map_location=device)

    # é‡å»ºæ¨¡å‹
    num_classes = checkpoint['num_classes']
    model = create_model(num_classes, pretrained=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)

    print(f"æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"  æ¶æ„: {checkpoint['model_architecture']}")
    print(f"  ç±»åˆ«æ•°: {num_classes}")
    print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint['best_val_acc']:.2f}%")
    print(f"  è®­ç»ƒæ—¶é—´: {checkpoint['timestamp']}")

    return model, checkpoint['classes'], checkpoint


def main():
    # é…ç½®å‚æ•°
    DATA_DIR = "/data2/lrf/data/UCMerced_LandUse/Images/Total_GT"
    SAVE_DIR = "/data2/lrf/HIIF/models/cls/results"
    BATCH_SIZE = 32
    NUM_EPOCHS = 50
    LEARNING_RATE = 0.001
    TRAIN_SPLIT = 0.7  # 70% è®­ç»ƒï¼Œ30% éªŒè¯

    print("ğŸš€ å¼€å§‹UCMercedåœŸåœ°åˆ©ç”¨åˆ†ç±»è®­ç»ƒ")
    print("=" * 80)
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {SAVE_DIR}")
    print(f"è®­ç»ƒ/éªŒè¯æ¯”ä¾‹: {int(TRAIN_SPLIT * 100)}% / {int((1 - TRAIN_SPLIT) * 100)}%")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
    print(f"å­¦ä¹ ç‡: {LEARNING_RATE}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        train_loader, val_loader, classes = create_data_loaders(
            DATA_DIR, BATCH_SIZE, TRAIN_SPLIT
        )
        print(f"\nâœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼")
        print(f"ç±»åˆ«æ•°: {len(classes)}")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_loader.dataset)}")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ”§ åˆ›å»ºResNet18æ¨¡å‹ï¼ˆä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡ï¼‰...")
    model = create_model(num_classes=len(classes), pretrained=True)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # è®­ç»ƒæ¨¡å‹
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    model, history = train_model(
        model, train_loader, val_loader, classes, SAVE_DIR, NUM_EPOCHS, LEARNING_RATE
    )

    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(history, SAVE_DIR)

    # ä¿å­˜ç±»åˆ«ä¿¡æ¯
    classes_info_path = os.path.join(SAVE_DIR, "class_info.txt")
    with open(classes_info_path, 'w') as f:
        f.write("UCMerced Land Use Classification - Class Information\n")
        f.write("=" * 50 + "\n")
        f.write(f"Total classes: {len(classes)}\n")
        f.write(f"Best validation accuracy: {history['best_val_acc']:.2f}%\n")
        f.write(f"Best model epoch: {history['best_epoch']}\n\n")
        f.write("Class mapping:\n")
        for i, class_name in enumerate(classes):
            f.write(f"{i:2d}: {class_name}\n")

    print(f"\nğŸ“ ç±»åˆ«ä¿¡æ¯å·²ä¿å­˜: {classes_info_path}")
    print(f"ğŸ“Š è®­ç»ƒå†å²å›¾åƒå·²ä¿å­˜: {os.path.join(SAVE_DIR, 'training_history.png')}")

    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒä»»åŠ¡å®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history['best_val_acc']:.2f}%")
    print(f"æœ€ä½³æ¨¡å‹è·¯å¾„: {history['best_model_path']}")
    print("=" * 80)


if __name__ == "__main__":
    main()