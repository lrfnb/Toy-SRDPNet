import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import re
from datetime import datetime
from collections import defaultdict
import pandas as pd


class UCMercedTestDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        """
        UCMercedæµ‹è¯•æ•°æ®é›†åŠ è½½å™¨
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½•
            transform: æ•°æ®é¢„å¤„ç†å˜æ¢
        """
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []
        self.classes = []
        self.class_to_idx = {}

        # æ‰«ææ•°æ®ç›®å½•ï¼Œæå–å›¾ç‰‡å’Œæ ‡ç­¾
        self._load_dataset()

    def _load_dataset(self):
        """åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼Œä»æ–‡ä»¶åæå–æ ‡ç­¾"""
        if not os.path.exists(self.data_dir):
            raise ValueError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")

        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = []
        for file in os.listdir(self.data_dir):
            if file.lower().endswith(('.tif', '.jpg', '.jpeg', '.png')):
                image_files.append(file)

        if len(image_files) == 0:
            raise ValueError(f"åœ¨ç›®å½• {self.data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")

        # ä»æ–‡ä»¶åæå–ç±»åˆ«æ ‡ç­¾
        class_names = set()
        for filename in image_files:
            # æå–ç±»åˆ«åï¼ˆå»æ‰æ•°å­—å’Œæ‰©å±•åï¼‰
            # ä¾‹å¦‚: agricultural00.tif -> agricultural
            class_name = re.sub(r'\d+\..*$', '', filename)
            class_names.add(class_name)

        # åˆ›å»ºç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„
        self.classes = sorted(list(class_names))
        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.classes)}

        print(f"æµ‹è¯•é›†å‘ç° {len(self.classes)} ä¸ªç±»åˆ«: {self.classes}")

        # åˆ›å»ºæ ·æœ¬åˆ—è¡¨
        for filename in image_files:
            class_name = re.sub(r'\d+\..*$', '', filename)
            if class_name in self.class_to_idx:  # ç¡®ä¿ç±»åˆ«å­˜åœ¨
                class_idx = self.class_to_idx[class_name]
                image_path = os.path.join(self.data_dir, filename)
                self.samples.append((image_path, class_idx, filename))

        print(f"æ€»å…±åŠ è½½äº† {len(self.samples)} ä¸ªæµ‹è¯•æ ·æœ¬")

        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        class_counts = defaultdict(int)
        for _, label, _ in self.samples:
            class_counts[self.classes[label]] += 1

        print("å„ç±»åˆ«æµ‹è¯•æ ·æœ¬æ•°ç»Ÿè®¡:")
        for class_name, count in sorted(class_counts.items()):
            print(f"  {class_name}: {count}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image_path, label, filename = self.samples[idx]

        # åŠ è½½å›¾ç‰‡
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"æ— æ³•åŠ è½½å›¾ç‰‡ {image_path}: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„é»‘è‰²å›¾ç‰‡
            image = Image.new('RGB', (256, 256), (0, 0, 0))

        if self.transform:
            image = self.transform(image)

        return image, label, filename


def create_model(num_classes):
    """åˆ›å»ºResNet18æ¨¡å‹æ¶æ„"""
    model = models.resnet18(pretrained=False)  # ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡ï¼Œä¼šåŠ è½½æˆ‘ä»¬çš„æƒé‡
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model


def load_model(model_path, device=None):
    """åŠ è½½ä¿å­˜çš„æ¨¡å‹"""
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    checkpoint = torch.load(model_path, map_location=device)

    # é‡å»ºæ¨¡å‹
    num_classes = checkpoint['num_classes']
    model = create_model(num_classes)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"  æ¶æ„: {checkpoint.get('model_architecture', 'resnet18')}")
    print(f"  ç±»åˆ«æ•°: {num_classes}")
    print(f"  è®­ç»ƒæ—¶æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint['best_val_acc']:.2f}%")
    print(f"  è®­ç»ƒæ—¶é—´: {checkpoint.get('timestamp', 'Unknown')}")
    print(f"  è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'Unknown')}")

    return model, checkpoint['classes'], checkpoint


def create_test_dataloader(data_dir, batch_size=32):
    """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    # æµ‹è¯•æ—¶çš„æ•°æ®é¢„å¤„ç†ï¼ˆä¸åŒ…å«æ•°æ®å¢å¼ºï¼‰
    test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    test_dataset = UCMercedTestDataset(data_dir, transform=test_transform)
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False,
        num_workers=4, pin_memory=True
    )

    return test_loader, test_dataset.classes


def test_model(model, test_loader, device, trained_classes):
    """æµ‹è¯•æ¨¡å‹å¹¶æ”¶é›†è¯¦ç»†ç»“æœ"""
    model.eval()

    all_predictions = []
    all_labels = []
    all_filenames = []
    all_probabilities = []

    correct = 0
    total = 0

    print("å¼€å§‹æµ‹è¯•...")
    print("-" * 60)

    with torch.no_grad():
        for batch_idx, (data, target, filenames) in enumerate(test_loader):
            data, target = data.to(device), target.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(data)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)

            # æ”¶é›†ç»“æœ
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(target.cpu().numpy())
            all_filenames.extend(filenames)
            all_probabilities.extend(probabilities.cpu().numpy())

            # è®¡ç®—å‡†ç¡®ç‡
            total += target.size(0)
            correct += (predicted == target).sum().item()

            if (batch_idx + 1) % 10 == 0:
                current_acc = 100.0 * correct / total
                print(f"å·²å¤„ç† {total} ä¸ªæ ·æœ¬ï¼Œå½“å‰å‡†ç¡®ç‡: {current_acc:.2f}%")

    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    final_accuracy = 100.0 * correct / total
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"æ­£ç¡®é¢„æµ‹: {correct}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {final_accuracy:.2f}%")

    return {
        'predictions': all_predictions,
        'labels': all_labels,
        'filenames': all_filenames,
        'probabilities': all_probabilities,
        'accuracy': final_accuracy,
        'total_samples': total,
        'correct_samples': correct
    }


def calculate_metrics(results, class_names):
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
    y_true = results['labels']
    y_pred = results['predictions']

    # åŸºæœ¬æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average=None, labels=range(len(class_names))
    )

    # è®¡ç®—å®å¹³å‡å’Œå¾®å¹³å‡
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
        y_true, y_pred, average='macro'
    )

    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(
        y_true, y_pred, average='micro'
    )

    # è®¡ç®—åŠ æƒå¹³å‡
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        y_true, y_pred, average='weighted'
    )

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)

    return {
        'accuracy': accuracy,
        'precision_per_class': precision,
        'recall_per_class': recall,
        'f1_per_class': f1,
        'support_per_class': support,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'precision_micro': precision_micro,
        'recall_micro': recall_micro,
        'f1_micro': f1_micro,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted,
        'confusion_matrix': cm
    }


def save_detailed_results(results, metrics, class_names, model_info, save_dir):
    """ä¿å­˜è¯¦ç»†çš„æµ‹è¯•ç»“æœåˆ°txtæ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_filename = f"test_results_{timestamp}.txt"
    results_path = os.path.join(save_dir, results_filename)

    with open(results_path, 'w', encoding='utf-8') as f:
        f.write("UCMerced Land Use Classification - Test Results\n")
        f.write("=" * 80 + "\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ¨¡å‹æ–‡ä»¶: {model_info.get('model_path', 'Unknown')}\n")
        f.write(f"è®­ç»ƒæ—¶æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {model_info.get('best_val_acc', 'Unknown'):.2f}%\n")
        f.write(f"è®­ç»ƒè½®æ•°: {model_info.get('epoch', 'Unknown')}\n")
        f.write(f"è®­ç»ƒæ—¶é—´: {model_info.get('timestamp', 'Unknown')}\n")
        f.write("\n" + "=" * 80 + "\n")

        # æ€»ä½“ç»“æœ
        f.write("æ€»ä½“æµ‹è¯•ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        f.write(f"æ€»æµ‹è¯•æ ·æœ¬æ•°: {results['total_samples']}\n")
        f.write(f"æ­£ç¡®é¢„æµ‹æ•°: {results['correct_samples']}\n")
        f.write(f"æµ‹è¯•å‡†ç¡®ç‡: {results['accuracy']:.4f} ({results['accuracy']:.2f}%)\n")
        f.write("\n")

        # å¹³å‡æŒ‡æ ‡
        f.write("å¹³å‡æŒ‡æ ‡:\n")
        f.write("-" * 40 + "\n")
        f.write(f"ç²¾ç¡®ç‡ (Macro Average): {metrics['precision_macro']:.4f}\n")
        f.write(f"å¬å›ç‡ (Macro Average): {metrics['recall_macro']:.4f}\n")
        f.write(f"F1åˆ†æ•° (Macro Average): {metrics['f1_macro']:.4f}\n")
        f.write(f"ç²¾ç¡®ç‡ (Micro Average): {metrics['precision_micro']:.4f}\n")
        f.write(f"å¬å›ç‡ (Micro Average): {metrics['recall_micro']:.4f}\n")
        f.write(f"F1åˆ†æ•° (Micro Average): {metrics['f1_micro']:.4f}\n")
        f.write(f"ç²¾ç¡®ç‡ (Weighted Average): {metrics['precision_weighted']:.4f}\n")
        f.write(f"å¬å›ç‡ (Weighted Average): {metrics['recall_weighted']:.4f}\n")
        f.write(f"F1åˆ†æ•° (Weighted Average): {metrics['f1_weighted']:.4f}\n")
        f.write("\n")

        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        f.write("å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'ç±»åˆ«':<15} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10} {'æ”¯æŒæ•°':<10} {'å‡†ç¡®ç‡':<10}\n")
        f.write("-" * 80 + "\n")

        for i, class_name in enumerate(class_names):
            # è®¡ç®—è¯¥ç±»åˆ«çš„å‡†ç¡®ç‡
            class_correct = sum(1 for true_label, pred_label in zip(results['labels'], results['predictions'])
                                if true_label == i and pred_label == i)
            class_total = sum(1 for label in results['labels'] if label == i)
            class_accuracy = class_correct / class_total if class_total > 0 else 0

            f.write(f"{class_name:<15} {metrics['precision_per_class'][i]:<10.4f} "
                    f"{metrics['recall_per_class'][i]:<10.4f} {metrics['f1_per_class'][i]:<10.4f} "
                    f"{metrics['support_per_class'][i]:<10} {class_accuracy:<10.4f}\n")

        f.write("\n")

        # æ··æ·†çŸ©é˜µ
        f.write("æ··æ·†çŸ©é˜µ (è¡Œ:çœŸå®æ ‡ç­¾, åˆ—:é¢„æµ‹æ ‡ç­¾):\n")
        f.write("-" * 80 + "\n")

        # è¡¨å¤´
        f.write(f"{'ç±»åˆ«':<12}")
        for class_name in class_names:
            f.write(f"{class_name[:8]:<8}")
        f.write("\n")
        f.write("-" * (12 + 8 * len(class_names)) + "\n")

        # æ··æ·†çŸ©é˜µæ•°æ®
        cm = metrics['confusion_matrix']
        for i, class_name in enumerate(class_names):
            f.write(f"{class_name[:12]:<12}")
            for j in range(len(class_names)):
                f.write(f"{cm[i][j]:<8}")
            f.write("\n")

        f.write("\n")

        # åˆ†ç±»æŠ¥å‘Š
        f.write("Sklearnåˆ†ç±»æŠ¥å‘Š:\n")
        f.write("-" * 80 + "\n")
        report = classification_report(results['labels'], results['predictions'],
                                       target_names=class_names, digits=4)
        f.write(report)
        f.write("\n")

        # é¢„æµ‹é”™è¯¯çš„æ ·æœ¬
        f.write("é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ (ä»…æ˜¾ç¤ºå‰50ä¸ª):\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'æ–‡ä»¶å':<25} {'çœŸå®æ ‡ç­¾':<15} {'é¢„æµ‹æ ‡ç­¾':<15} {'ç½®ä¿¡åº¦':<10}\n")
        f.write("-" * 80 + "\n")

        error_count = 0
        for i, (filename, true_label, pred_label, prob) in enumerate(
                zip(results['filenames'], results['labels'], results['predictions'], results['probabilities'])
        ):
            if true_label != pred_label and error_count < 50:
                confidence = prob[pred_label]
                f.write(f"{filename[:25]:<25} {class_names[true_label]:<15} "
                        f"{class_names[pred_label]:<15} {confidence:<10.4f}\n")
                error_count += 1

        if error_count == 50:
            total_errors = sum(1 for true_label, pred_label in zip(results['labels'], results['predictions'])
                               if true_label != pred_label)
            f.write(f"... (æ€»å…± {total_errors} ä¸ªé”™è¯¯é¢„æµ‹ï¼Œä»…æ˜¾ç¤ºå‰50ä¸ª)\n")

    print(f"ğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    return results_path


def save_confusion_matrix_plot(confusion_matrix, class_names, save_dir):
    """ä¿å­˜æ··æ·†çŸ©é˜µå¯è§†åŒ–å›¾"""
    plt.figure(figsize=(12, 10))

    # ä½¿ç”¨çƒ­åŠ›å›¾æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'})

    plt.title('Confusion Matrix - UCMerced Land Use Classification', fontsize=16, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=14)
    plt.ylabel('True Label', fontsize=14)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)

    plt.tight_layout()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_path = os.path.join(save_dir, f"confusion_matrix_{timestamp}.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"ğŸ“Š æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜åˆ°: {plot_path}")
    return plot_path


def save_results_csv(results, class_names, save_dir):
    """ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSVæ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = os.path.join(save_dir, f"detailed_predictions_{timestamp}.csv")

    # åˆ›å»ºDataFrame
    data = []
    for i, (filename, true_label, pred_label, prob) in enumerate(
            zip(results['filenames'], results['labels'], results['predictions'], results['probabilities'])
    ):
        row = {
            'filename': filename,
            'true_class': class_names[true_label],
            'predicted_class': class_names[pred_label],
            'correct': true_label == pred_label,
            'confidence': prob[pred_label],
            'true_class_prob': prob[true_label]
        }

        # æ·»åŠ æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        for j, class_name in enumerate(class_names):
            row[f'prob_{class_name}'] = prob[j]

        data.append(row)

    df = pd.DataFrame(data)
    df.to_csv(csv_path, index=False, encoding='utf-8')

    print(f"ğŸ“Š è¯¦ç»†é¢„æµ‹ç»“æœCSVå·²ä¿å­˜åˆ°: {csv_path}")
    return csv_path


def main():
    # é…ç½®å‚æ•°
    MODEL_PATH = "/data2/lrf/HIIF/models/cls/results/resnet18_ucmerced_best.pth"
    TEST_DATA_DIR = "/data2/lrf/IDM/experiments/uc_x8"
    SAVE_DIR = "/data2/lrf/HIIF/models/cls/results/idm"
    BATCH_SIZE = 32

    print("ğŸ§ª UCMercedåœŸåœ°åˆ©ç”¨åˆ†ç±» - æ¨¡å‹æµ‹è¯•")
    print("=" * 80)
    print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"æµ‹è¯•æ•°æ®è·¯å¾„: {TEST_DATA_DIR}")
    print(f"ç»“æœä¿å­˜è·¯å¾„: {SAVE_DIR}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")

    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        return

    if not os.path.exists(TEST_DATA_DIR):
        print(f"âŒ æµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨: {TEST_DATA_DIR}")
        return

    os.makedirs(SAVE_DIR, exist_ok=True)

    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    try:
        # åŠ è½½æ¨¡å‹
        print(f"\nğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
        model, trained_classes, checkpoint = load_model(MODEL_PATH, device)

        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
        test_loader, test_classes = create_test_dataloader(TEST_DATA_DIR, BATCH_SIZE)

        # æ£€æŸ¥ç±»åˆ«æ˜¯å¦åŒ¹é…
        print(f"\nğŸ” æ£€æŸ¥ç±»åˆ«åŒ¹é…æ€§...")
        print(f"è®­ç»ƒæ—¶çš„ç±»åˆ«æ•°: {len(trained_classes)}")
        print(f"æµ‹è¯•æ•°æ®çš„ç±»åˆ«æ•°: {len(test_classes)}")

        if set(trained_classes) != set(test_classes):
            print("âš ï¸  è­¦å‘Š: è®­ç»ƒå’Œæµ‹è¯•çš„ç±»åˆ«ä¸å®Œå…¨åŒ¹é…!")
            print(f"è®­ç»ƒç±»åˆ«: {sorted(trained_classes)}")
            print(f"æµ‹è¯•ç±»åˆ«: {sorted(test_classes)}")
        else:
            print("âœ… è®­ç»ƒå’Œæµ‹è¯•ç±»åˆ«å®Œå…¨åŒ¹é…!")

        # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç±»åˆ«é¡ºåº
        class_names = trained_classes

        # è¿›è¡Œæµ‹è¯•
        print(f"\nğŸ§ª å¼€å§‹æ¨¡å‹æµ‹è¯•...")
        results = test_model(model, test_loader, device, trained_classes)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        print(f"\nğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = calculate_metrics(results, class_names)

        # å‡†å¤‡æ¨¡å‹ä¿¡æ¯
        model_info = {
            'model_path': MODEL_PATH,
            'best_val_acc': checkpoint.get('best_val_acc', 0),
            'epoch': checkpoint.get('epoch', 'Unknown'),
            'timestamp': checkpoint.get('timestamp', 'Unknown')
        }

        # ä¿å­˜è¯¦ç»†ç»“æœ
        print(f"\nğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ...")
        results_path = save_detailed_results(results, metrics, class_names, model_info, SAVE_DIR)

        # ä¿å­˜æ··æ·†çŸ©é˜µå›¾
        confusion_matrix_path = save_confusion_matrix_plot(metrics['confusion_matrix'], class_names, SAVE_DIR)

        # ä¿å­˜CSVç»“æœ
        csv_path = save_results_csv(results, class_names, SAVE_DIR)

        # æ‰“å°æ€»ç»“
        print(f"\n" + "=" * 80)
        print(f"ğŸ‰ æµ‹è¯•å®Œæˆ! ç»“æœæ€»ç»“:")
        print(f"ğŸ“ˆ æµ‹è¯•å‡†ç¡®ç‡: {results['accuracy']:.2f}%")
        print(f"ğŸ“Š F1åˆ†æ•° (å®å¹³å‡): {metrics['f1_macro']:.4f}")
        print(f"ğŸ“Š F1åˆ†æ•° (åŠ æƒå¹³å‡): {metrics['f1_weighted']:.4f}")
        print(f"ğŸ“ è¯¦ç»†ç»“æœ: {results_path}")
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µ: {confusion_matrix_path}")
        print(f"ğŸ“‹ CSVæ–‡ä»¶: {csv_path}")
        print("=" * 80)

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()